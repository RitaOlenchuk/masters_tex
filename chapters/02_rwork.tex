\chapter{Related work}
\section{Deep learning-based semantic segmentation}
Semantic segmentation is a computer vision task that aims to differentiate regions by assigning a class label to each pixel. Due to the success of deep learning models in a wide range of vision applications, various deep learning-based algorithms for image segmentation have been developed and published in the literature~\cite{haralick1983image}. One of the most prominent deep learning architectures used by the computer vision community include fully convolutional networks (FCNs)~\cite{long2015fully}, encoder-decoders~\cite{noh2015learning}, recurrent neural networks (RNNs), and generative adversarial networks (GANs)~\cite{goodfellow2014generative}.

FCNs~\cite{long2015fully} are among the most widely used architectures for computer vision tasks and their general architecture consists of several learnable convolutions, pooling layers, and a final 1$\times$1 convolution. While models based on this architecture perform well on challenging segmentation benchmarks, e.g. applied on scene segmentation \cite{yu2020context} and instance aware semantic segmentation~\cite{li2017fully}, they are also used on segmentation problems in histology domain such as colon glands segmentation~\cite{bentaieb2016topology}, identification of muscle and messy regions in contexts of inflammatory bowel disease~\cite{wang2016deep} and nuclei segmentation for breast cancer~\cite{natarajan2020segmentation} all performed on the Hematoxylin and Eosin (H\&E) stained histopathology images. However, despite its popularity, the conventional FCN model has limitations such as loss of localization and the inability to process potentially useful global context information due to a series of down-sampling and a high sampling rate.

A popular group of deep learning models for semantic image segmentation that aims to solve the aforementioned issues of FCNs is based on the convolutional encoder-decoder architecture~\cite{noh2015learning}. Their model consists of two parts, an encoder consisting of convolutional layers and a deconvolution network that consists of deconvolution and unpooling layers that take the feature vector as input and generate a map of pixel-wise class probabilities. Example for such a convolutional encoder-decoder architecture for image segmentation is SegNet~\cite{badrinarayanan2017segnet}. The SegNet's encoder network has 13 convolutional layers with corresponding layers in the decoder. The final decoder output is fed to a multi-class soft-max classifier to produce class probabilities for each pixel independently. The main feature of SegNet is that the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This allows it to achieve high scores for road scene understanding problems~\cite{badrinarayanan2017segnet}, COVID-19 lung computed tomography image segmentation~\cite{saood2021covid}, liver tumor segmentation in computed tomography scans~\cite{almotairi2020liver} and colon cancer histopathological images analysis~\cite{hamida2021deep}. There are several encoder-decoder models initially developed for biomedical image segmentation. Ronneberger et al.~\cite{10.1007/978-3-319-24574-4_28} proposed the U-Net model for segmenting biological microscopy images that can train with few annotated images effectively. U-Net has an FCN-like down-sampling part that extracts features with 3$\times$3 convolutions and an up-sampling part. Feature maps from the encoder are copied to the corresponding decoder part of the network to avoid losing pattern information. Besides the segmentation of neuronal structures in electron microscopic recordings demonstrated in the original paper~\cite{10.1007/978-3-319-24574-4_28}, U-Net was applied for numerous further tasks such as nuclei segmentation in histology images~\cite{zeng2019ric}, segmenting individual colon glands in histopathology images~\cite{pinckaers2019neural}, epidermal tissue segmentation in histopathological images of skin biolsies~\cite{oskal2019u} and cell segmentation on histopathology triple-negative breast cancer patients dataset~\cite{bagdigen2020cell}. A further example of an encoder-decoder model for semantic segmentation of histopathology images is HookNet~\cite{van2021hooknet}. The architecture consists of two encoder-decoder branches to extract contextual and fine-grained detailed information and combine it (hook up) for the target segmentation. The model showed improvement compared with single-resolution models and was applied to segment different histopathologies like breast cancer tissue sections~\cite{van2021hooknet}, lung squamous cell carcinoma~\cite{van2021hooknet}, invasive melanoma tumor~\cite{shahdeep} and cervical cancer~\cite{meng2021cervical} slides.

Another widely used group of deep learning models for semantic segmentation are the atrous (or dilated) convolutional models that include the DeepLab family~\cite{chen2017deeplab, chen2017rethinking}. The use of atrous convolutions addresses the decreasing resolution caused by max-pooling and striding and Atrous Spatial Pyramid Pooling (ASPP) analyzes an incoming convolutional feature layer with filters at multiple sampling rates allowing to capture objects and image context at multiple scales to robustly segment objects at multiple scales. DeepLabv3+~\cite{chen2018encoder} uses encoder-decoder architecture including atrous separable convolution, composed of a depthwise convolution (spatial convolution for each channel of the input) and pointwise convolution (1$\times$1 convolution with the depthwise convolution as input). Authors~\cite{chen2018encoder} demonstrated the effectiveness of DeppLabv3+ model with modified Xception backbone at recognition of visual object classes in realistic scenes, but it also found multiple applications such as skin lesion segmentation~\cite{azad2020attention}, segmentation of H\&E stained breast cancer~\cite{priego2020automatic} and colorectal carcinoma~\cite{xu2022spatial} histopathology images. Despite all the efforts, even this popular architecture has constraints in learning long-range dependency and spatial correlations due to the inductive bias of locality and weight sharing~\cite{xie2021cotr} that may result in sub-optimal segmentation of complex structures. 

The general architecture of GANs~\cite{goodfellow2014generative} consists of the discriminator and the generator. The generator learns the real data distribution and produces similar data, while the discriminator discriminates between real data and simulated data. Hence the task of the generator is to learn to generate the best images to fool the discriminator. There are many extended models such as conditional GAN (cGAN)~\cite{mirza2014conditional} where the additional information is added to both the generator and the discriminator as a condition. This architecture was used for semantic segmentation of brain tumor in magnetic resonance imaging~\cite{rezaei2017conditional} and nuclei segmentation in histopathology images~\cite{mahmood2019deep}. Further extended version of cGAN, pix2pix~\cite{isola2017image} was developed for conversion between different types of images but also found use cases in medical setting such as cell image segmentation on the fluorescence liver images~\cite{Tsuda_2019_CVPR_Workshops} and retinal blood vessel segmentation~\cite{popescu2021retinal}. A further GAN extension originally developed for image transformation between two domains but also applicable for segmentation is CycleGAN~\cite{zhu2017unpaired}. The architecture has two mirror-symmetric GANs to form a ring network to find the mapping between domains. For instance, CycleGAN was applied to kidney tissue ~\cite{gadermayr2019generative} segmentation. Some GAN-based models were specifically developed for semantic segmentation in the medical domain, such as Domain Adaptation and Segmentation GAN (DASGAN)~\cite{kapil2019dasgan} that performs image-to-image translation and semantic tumor epithelium segmentation. It has an extended CycleGAN architecture with discriminator networks adjusted to predict pixel-wise class probability maps on top of predicting the correct source of an image. As a further example the proposed architecture consisting of pyramid of GAN structures~\cite{li2022high}, each responsible for generating and segmenting images at a different scale, was applied to segment prostate histopathology images.

%The progress in Natural Language Processing (NLP) provided several alternative aggregation schemes based on various attention strategies \cite{fu2020scene, fu2019dual} that can be used in the context of segmentation to better capture contextual information. Transformers \cite{vaswani2017attention} are state-of-the art for many NLP tasks. They rely on self-attention mechanisms and capture long-range dependencies among tokens (words) in a sentence. Some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet \cite{hu2019local}. Another usage in the context of segmentation would be the application of the self-attention layers as an addition to backbones \cite{bello2019attention, cao2019gcnet, yin2020disentangled} or head networks \cite{gu2018learning, hu2018relation} by providing the capability to encode distant dependencies or heterogeneous interactions.
%Transformer-based models have also gained a lot of attraction in medical image analysis. Transformer bottleneck was added to a 2D U-Net architecture for Multi-Atlas Abdomen Labeling Challenge \cite{chen2021transunet}, Transformer model was also fed with feature maps to extend the approach of 3D brain tumor segmentation \cite{wang2021transbts}.
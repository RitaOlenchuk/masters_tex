\chapter{Related work}
\section{Deep learning-based semantic segmentation}
Semantic segmentation is a computer vision task that aims to differentiate regions by assigning a class label to each pixel. Due to the success of deep learning models in a wide range of vision applications, various deep learning-based algorithms for image segmentation have been developed and published in the literature~\cite{haralick1983image}. One of the most prominent deep learning architectures used by the computer vision community include fully convolutional networks (FCNs)~\cite{long2015fully}, encoder-decoders~\cite{noh2015learning}, recurrent neural networks (RNNs), and long short term memory (LSTM), and generative adversarial networks (GANs).

FCNs are among the most widely used architectures for computer vision tasks and their general architecture consists of several learnable convolutions, pooling layers, and final 1$\times$1 convolution. While models based on this architecture perform well on challenging segmentation benchmarks, e.g. applied on scene segmentation \cite{yu2020context} and instance aware semantic segmentation~\cite{li2017fully}, they are also used on segmentation problems in histology domain such as colon glands segmentation on Hematoxylin and Eosin (H\&E) stained slides~\cite{bentaieb2016topology}, identification of muscle and messy regions on H\&E stained slides in contexts of inflammatory bowel disease~\cite{wang2016deep} and nuclei segmentation in the H\&E stained breast cancer histopathology images~\cite{natarajan2020segmentation}. However, despite its popularity, the conventional FCN model has limitations such as loss of localization and the inability to process potentially useful global context information due to a series of down-sampling and a high sampling rate.

A popular group of deep learning models for semantic image segmentation that aims to solve the aforementioned issues of FCNs is based on the convolutional encoder-decoder architecture. Their model consists of two parts, an encoder consisting of convolutional layers and a deconvolution network that consists of deconvolution and unpooling layers that take the feature vector as input and generate a map of pixel-wise class probabilities. Example for such a convolutional encoder-decoder architecture for image segmentation is SegNet~\cite{badrinarayanan2017segnet}. The SegNet's encoder network has 13 convolutional layers with corresponding layers in the decoder. The final decoder output is fed to a multi-class soft-max classifier to produce class probabilities for each pixel independently. The main feature of SegNet is that the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This allows it to achieve high scores for road scene understanding problems~\cite{badrinarayanan2017segnet}, COVID-19 lung computed tomography image segmentation~\cite{saood2021covid}, liver tumor segmentation in computed tomography scans~\cite{almotairi2020liver} and colon cancer histopathological images analysis~\cite{hamida2021deep}. There are several encoder-decoder models initially developed for biomedical image segmentation. Ronneberger et al.~\cite{10.1007/978-3-319-24574-4_28} proposed the U-Net model for segmenting biological microscopy images that can train with few annotated images effectively. U-Net has an FCN-like down-sampling part that extracts features with 3$\times$3 convolutions and an up-sampling part. Feature maps from the encoder are copied to the corresponding decoder part of the network to avoid losing pattern information. Besides the segmentation of neuronal structures in electron microscopic recordings demonstrated in the original paper~\cite{10.1007/978-3-319-24574-4_28}, U-Net was applied for numerous further tasks such as nuclei segmentation in histology images~\cite{zeng2019ric}, segmenting individual colon glands in histopathology images~\cite{pinckaers2019neural}, epidermal tissue segmentation in histopathological images of skin biolsies~\cite{oskal2019u} and cell segmentation on histopathology triple-negative breast cancer patients dataset~\cite{bagdigen2020cell}.

Another popular family of deep learning models for semantic segmentation are the atrous (or dilated) convolutional models that include the DeepLab family~\cite{chen2017deeplab, chen2017rethinking}. The use of atrous convolutions addresses the decreasing resolution caused by max-pooling and striding and Atrous Spatial Pyramid Pooling (ASPP) analyzes an incoming convolutional feature layer with filters at multiple sampling rates allowing to capture objects and image context at multiple scales to robustly segment objects at multiple scales. DeepLabv3+~\cite{chen2018encoder} uses encoder-decoder architecture including atrous separable convolution, composed of a depthwise convolution (spatial comvolution for each channel of the input) and pointwise convolution (1$\times$1 comvolution with the depthwise convolution as input). Authors demonstrated the effectiveness of DeppLabv3+ model with modified Xception backbone applied to PASCAL VOC challenge data, but it also found multiple applications such as in skin lesion segmentation~\cite{azad2020attention}. 

%The progress in Natural Language Processing (NLP) provided several alternative aggregation schemes based on various attention strategies \cite{fu2020scene, fu2019dual} that can be used in the context of segmentation to better capture contextual information. Transformers \cite{vaswani2017attention} are state-of-the art for many NLP tasks. They rely on self-attention mechanisms and capture long-range dependencies among tokens (words) in a sentence. Some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet \cite{hu2019local}. Another usage in the context of segmentation would be the application of the self-attention layers as an addition to backbones \cite{bello2019attention, cao2019gcnet, yin2020disentangled} or head networks \cite{gu2018learning, hu2018relation} by providing the capability to encode distant dependencies or heterogeneous interactions.
%Transformer-based models have also gained a lot of attraction in medical image analysis. Transformer bottleneck was added to a 2D U-Net architecture for Multi-Atlas Abdomen Labeling Challenge \cite{chen2021transunet}, Transformer model was also fed with feature maps to extend the approach of 3D brain tumor segmentation \cite{wang2021transbts}.